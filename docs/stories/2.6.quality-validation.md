# Story 2.6: Text Extraction Quality Validation and Testing

## Status
Draft

## Story
**As a** researcher,
**I want** automated and manual validation of text extraction quality,
**so that** I can verify the system meets 95%+ accuracy targets.

## Acceptance Criteria
1. Quality validation utilities created in utils/quality_validation.py
2. Spot-checking script randomly selects N screenshots (default: 10) from ingested book
3. For each selected screenshot, script displays: original screenshot image, extracted text side-by-side
4. Manual validation prompt: "Rate accuracy (1-10): ", "Note any major errors: "
5. Validation results logged with: screenshot_id, accuracy_rating, notes, timestamp
6. Aggregate quality report generated: average accuracy score, confidence level (based on sample size), error patterns
7. If average accuracy <9.5 (equivalent to 95%), script flags for review and suggests: try different vision model, adjust prompts, review edge cases
8. Test suite (pytest) includes integration test: ingest sample book → validate chunks exist → spot-check 3 random extractions programmatically
9. Sample "golden" test cases created: 5 screenshots with pre-verified correct text for automated regression testing
10. Successfully validates 3 diverse books (different layouts, fonts, formatting) with 95%+ accuracy
11. Error pattern analysis identifies common failures (if any): tables, images, footnotes, special formatting
12. Documentation updated with known limitations and workarounds

## Tasks / Subtasks
- [ ] Create quality validation utilities module (AC: 1, 2)
  - [ ] Create `minerva/utils/quality_validation.py` file
  - [ ] Define QualityValidator class
  - [ ] Implement `select_random_screenshots(book_id, n=10)` method
  - [ ] Add type hints and docstrings

- [ ] Implement screenshot and text display (AC: 3)
  - [ ] Create method to load screenshot image from file path
  - [ ] Fetch extracted text for screenshot from chunks
  - [ ] Display screenshot image using terminal image display or save to temp location
  - [ ] Display extracted text side-by-side or below image

- [ ] Implement manual validation workflow (AC: 4, 5)
  - [ ] Prompt user: "Rate accuracy (1-10): "
  - [ ] Validate input (1-10 range)
  - [ ] Prompt user: "Note any major errors: " (optional text input)
  - [ ] Store validation result: screenshot_id, accuracy_rating, notes, timestamp
  - [ ] Save results to JSON file or database table

- [ ] Implement quality reporting (AC: 6, 7)
  - [ ] Calculate average accuracy score from all ratings
  - [ ] Calculate confidence level based on sample size (e.g., margin of error)
  - [ ] Identify common error patterns from notes (keyword analysis)
  - [ ] Generate quality report with summary stats
  - [ ] Flag if average accuracy <9.5 (95%)
  - [ ] Provide improvement suggestions: different vision model, prompt adjustments, edge case handling

- [ ] Create CLI command for quality validation (AC: 3, 4, 5, 6, 7)
  - [ ] Add `minerva validate-quality --book-id <uuid>` command to cli/app.py
  - [ ] Add optional --sample-size parameter (default: 10)
  - [ ] Run validation workflow: select screenshots, display, prompt, report
  - [ ] Display final quality report

- [ ] Create automated golden test cases (AC: 9)
  - [ ] Create `tests/fixtures/golden_screenshots/` directory
  - [ ] Add 5 diverse screenshot test cases
  - [ ] Create corresponding expected_text files for each screenshot
  - [ ] Test cases: simple text, headers/lists, complex formatting, tables, footnotes

- [ ] Implement automated regression tests (AC: 8, 9)
  - [ ] Create `tests/integration/test_quality_validation.py`
  - [ ] Test: ingest sample book, validate chunks exist
  - [ ] Test: extract text from 3 golden screenshots, compare to expected text
  - [ ] Test: calculate accuracy score (e.g., Levenshtein distance or exact match)
  - [ ] Fail test if accuracy <95%

- [ ] Validate with diverse books (AC: 10, 11)
  - [ ] Manually test with 3 different books (different layouts, fonts, formatting)
  - [ ] Run quality validation for each book
  - [ ] Document accuracy scores for each book
  - [ ] Identify common failure patterns: tables, images, footnotes, special formatting
  - [ ] Log findings in validation results

- [ ] Update documentation (AC: 12)
  - [ ] Document known limitations in README.md or docs/
  - [ ] Add workarounds for common issues (e.g., tables, footnotes)
  - [ ] Update architecture docs with quality metrics
  - [ ] Add troubleshooting guide for low accuracy

## Dev Notes

### Quality Validation Process
[Source: architecture/test-strategy-and-standards.md]

**Manual Spot-Checking Process:**
1. Randomly select N screenshots from ingested book (default: 10)
2. For each screenshot:
   - Display original screenshot image
   - Display extracted text from chunks
   - Human reviewer rates accuracy (1-10 scale)
   - Human reviewer notes major errors (optional)
3. Calculate average accuracy score (rating / 10 * 100%)
4. Generate quality report with suggestions

**Accuracy Target:**
- 95%+ accuracy (average rating ≥9.5/10)
- Defined in PRD NFR1: "Text extraction accuracy shall meet or exceed 95%"

**Why Manual Validation:**
- Automated OCR/text comparison difficult for complex layouts
- Human judgment required for semantic accuracy
- Spot-checking balances thoroughness with efficiency

### File Location
[Source: architecture/source-tree.md]
- Module path: `minerva/utils/quality_validation.py`
- CLI command: `minerva/cli/app.py` (add validate-quality command)
- Test path: `tests/integration/test_quality_validation.py`
- Golden fixtures: `tests/fixtures/golden_screenshots/`

### Terminal Image Display
[Source: architecture/tech-stack.md]

**Options for Displaying Screenshots:**
1. **Rich + PIL**: Use Rich to display images in terminal (limited support)
2. **Save to temp file**: Save screenshot to temp location, print file path for user to open
3. **External viewer**: Open image in system default viewer (e.g., `open` on macOS)

**Recommended Approach:**
```python
import subprocess
from pathlib import Path

def display_screenshot(file_path: Path):
    # Open in system default viewer
    subprocess.run(["open", str(file_path)])  # macOS
    # Or: subprocess.run(["xdg-open", str(file_path)])  # Linux
```

### Data Model for Validation Results
[Source: architecture/data-models.md]

**Option 1: Store in database (ingestion_logs table)**
```python
# Log validation result
await log_ingestion_event(
    book_id=book_id,
    log_level="INFO",
    message=f"Quality validation: screenshot {screenshot_id}, accuracy {rating}/10",
    metadata={"screenshot_id": screenshot_id, "rating": rating, "notes": notes}
)
```

**Option 2: Store in JSON file**
```json
{
  "book_id": "uuid",
  "validation_date": "2025-10-06T12:00:00Z",
  "samples": [
    {
      "screenshot_id": "uuid",
      "accuracy_rating": 9,
      "notes": "Minor formatting issue with table",
      "timestamp": "2025-10-06T12:05:00Z"
    }
  ],
  "summary": {
    "average_accuracy": 9.2,
    "total_samples": 10,
    "flagged": true,
    "suggestions": ["Review table extraction", "Consider different vision model"]
  }
}
```

### Golden Test Cases
[Source: architecture/test-strategy-and-standards.md]

**Test Case Coverage:**
1. **Simple text page**: Plain paragraphs, no special formatting
2. **Headers and lists**: H1/H2 headers, bullet lists, numbered lists
3. **Complex formatting**: Bold, italics, quotes, mixed styles
4. **Tables**: Simple 2-3 column tables
5. **Footnotes**: Page with footnotes or citations

**Expected Accuracy:**
- Simple text: 98-100%
- Headers/lists: 95-98%
- Complex formatting: 90-95%
- Tables: 80-90% (known limitation)
- Footnotes: 85-95%

### Automated Accuracy Calculation
[Source: architecture/test-strategy-and-standards.md]

**Options:**
1. **Exact match**: Compare extracted text to expected text character-by-character
2. **Levenshtein distance**: Calculate edit distance, convert to similarity percentage
3. **Token-based similarity**: Compare tokens (words), calculate Jaccard similarity

**Recommended: Token-based similarity**
```python
def calculate_accuracy(extracted: str, expected: str) -> float:
    extracted_tokens = set(extracted.lower().split())
    expected_tokens = set(expected.lower().split())
    intersection = extracted_tokens & expected_tokens
    union = extracted_tokens | expected_tokens
    return len(intersection) / len(union) * 100  # Jaccard similarity
```

### Coding Standards
[Source: architecture/coding-standards.md]
- **Never use print() for logging** - Use structlog for quality validation logging
- **File paths must use pathlib.Path** - For screenshot file handling
- **All public functions must have type hints and docstrings**
- **Never access environment variables directly** - Use settings

### Error Pattern Analysis
[Source: architecture/test-strategy-and-standards.md]

**Common Failure Patterns to Document:**
1. **Tables**: Complex multi-column tables may have alignment issues
2. **Images**: Image captions may be missed or misplaced
3. **Footnotes**: Small text at bottom of page may have lower accuracy
4. **Special formatting**: Superscripts, subscripts, mathematical notation
5. **Multi-column layouts**: Text order may be incorrect

**Workarounds:**
- Tables: Manually review and correct table extractions
- Footnotes: Consider higher vision detail level ("high" instead of "low")
- Multi-column: Future enhancement to detect columns

### Testing

[Source: architecture/test-strategy-and-standards.md]

**Integration Test Framework:**
- pytest 7.4+ with pytest-asyncio
- Golden test fixtures in `tests/fixtures/golden_screenshots/`
- Location: `tests/integration/test_quality_validation.py`

**Test Cases Required:**
1. Golden test: Extract text from 5 golden screenshots, verify >95% accuracy
2. End-to-end test: Ingest sample book, validate chunks exist, spot-check extractions
3. Quality reporting: Verify average accuracy calculation, confidence level, error patterns
4. Flagging test: Verify script flags if accuracy <95%

**Sample Test Structure:**
```python
# tests/integration/test_quality_validation.py
import pytest
from minerva.utils.quality_validation import QualityValidator

@pytest.mark.asyncio
async def test_golden_screenshots(test_db):
    validator = QualityValidator(session=test_db)

    # Test each golden screenshot
    for screenshot_file, expected_text in golden_test_cases:
        extracted_text = await extract_text(screenshot_file)
        accuracy = calculate_accuracy(extracted_text, expected_text)
        assert accuracy >= 95.0, f"Accuracy {accuracy}% below 95% threshold"

@pytest.mark.asyncio
async def test_quality_validation_workflow(test_db):
    # Test full validation workflow
    pass
```

**Manual Testing:**
- Validate 3 diverse books manually
- Document accuracy for each book
- Identify common failure patterns

**CI Integration:**
- Integration tests run on every push via GitHub Actions
- Must pass: `pytest tests/integration/test_quality_validation.py -v`

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 1.0 | Initial story creation | Sarah (PO) |
| 2025-10-06 | 2.0 | Enhanced with comprehensive architecture details | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent_

### Debug Log References
_To be populated by dev agent_

### Completion Notes
_To be populated by dev agent_

### File List
_To be populated by dev agent_

## QA Results
_To be populated by QA agent_
