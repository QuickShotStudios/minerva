# Story 3.5: Export Script for Production Database

## Status
Ready for Review

## Story

**As a** researcher,
**I want** a script to export ingested books to production database,
**so that** I can make curated knowledge available to MyPeptidePal.ai without exposing local screenshots.

## Acceptance Criteria

1. Export script created: `minerva export --book-id <uuid>` command in CLI
2. Script validates: book exists, ingestion_status = "completed", embeddings present on all chunks
3. Pre-export report generated: book title, total chunks, total screenshots, estimated export size, warnings (if any)
4. User confirmation prompt: "Export [book title] to production? (y/n)" with summary displayed
5. SQL export file generated with: INSERT statements for book record (excluding local paths), all chunk records (including embeddings), screenshot metadata (id, sequence, hash ONLY - no file_path), embedding config record
6. Export file includes transaction wrapper: BEGIN; ... COMMIT; for atomic import
7. Export validates embedding config exists in production (or includes CREATE statement)
8. Screenshots explicitly excluded: file_path field set to NULL in exported data, clear comment in SQL file
9. Export file saved to: exports/{book_id}_{timestamp}.sql with proper formatting
10. Success message displays: export file path, import instructions ("Run this SQL against production DB")
11. Successfully exports test book: SQL file generated → import to test production DB → chunks queryable → embeddings intact
12. Optional --all flag exports all completed books in batch
13. Export script is idempotent: re-exporting same book generates valid SQL without duplicates (uses INSERT ... ON CONFLICT)

## Tasks / Subtasks

### Task 1: Create export CLI command (AC: 1)
- [ ] Add export command to `minerva/cli/app.py` (Typer CLI)
  ```python
  import typer
  from uuid import UUID
  from pathlib import Path

  app = typer.Typer()

  @app.command()
  def export(
      book_id: UUID = typer.Argument(..., help="UUID of book to export"),
      all: bool = typer.Option(False, "--all", help="Export all completed books"),
      output_dir: Path = typer.Option(Path("exports"), "--output-dir", help="Output directory for SQL files")
  ):
      """Export book(s) to production-ready SQL file."""
      asyncio.run(_export_book(book_id, all, output_dir))
  ```

### Task 2: Validate book and generate pre-export report (AC: 2, 3, 4)
- [ ] Create `minerva/core/export/export_service.py`
- [ ] Implement validation and report generation
  ```python
  from minerva.db.models.book import Book
  from minerva.db.models.chunk import Chunk
  from sqlalchemy import select, func

  async def validate_and_report(book_id: UUID, session: AsyncSession) -> dict:
      """Validate book ready for export and generate report."""

      # Fetch book
      book_query = select(Book).where(Book.id == book_id)
      result = await session.execute(book_query)
      book = result.scalar_one_or_none()

      if not book:
          raise ValueError(f"Book {book_id} not found")

      # Check ingestion status
      if book.ingestion_status != "completed":
          raise ValueError(
              f"Book not ready for export. Status: {book.ingestion_status}"
          )

      # Count chunks and validate embeddings
      chunks_query = select(Chunk).where(Chunk.book_id == book_id)
      chunks_result = await session.execute(chunks_query)
      chunks = chunks_result.scalars().all()

      # Check for missing embeddings
      missing_embeddings = [c.id for c in chunks if not c.embedding]
      if missing_embeddings:
          raise ValueError(
              f"{len(missing_embeddings)} chunks missing embeddings"
          )

      # Calculate export size (rough estimate)
      total_text_size = sum(len(c.chunk_text.encode()) for c in chunks)
      embedding_size = len(chunks) * 1536 * 4  # 1536 floats, 4 bytes each
      total_size_mb = (total_text_size + embedding_size) / (1024 * 1024)

      # Generate report
      report = {
          "book_id": str(book.id),
          "title": book.title,
          "author": book.author,
          "total_chunks": len(chunks),
          "total_screenshots": book.total_screenshots or 0,
          "estimated_size_mb": round(total_size_mb, 2),
          "warnings": []
      }

      # Add warnings if applicable
      if total_size_mb > 100:
          report["warnings"].append("Large export (>100MB)")

      return report
  ```
- [ ] Display report and get user confirmation
  ```python
  from rich.console import Console
  from rich.table import Table

  def display_export_report(report: dict) -> bool:
      """Display export report and prompt for confirmation."""
      console = Console()

      # Display report
      console.print("\n[bold cyan]Export Report[/bold cyan]\n")

      table = Table(show_header=False)
      table.add_row("Book Title", report["title"])
      table.add_row("Author", report["author"] or "Unknown")
      table.add_row("Total Chunks", str(report["total_chunks"]))
      table.add_row("Total Screenshots", str(report["total_screenshots"]))
      table.add_row("Estimated Size", f"{report['estimated_size_mb']} MB")

      console.print(table)

      # Show warnings
      if report["warnings"]:
          console.print("\n[bold yellow]Warnings:[/bold yellow]")
          for warning in report["warnings"]:
              console.print(f"  ⚠️  {warning}")

      # Confirm
      console.print(f"\nExport '{report['title']}' to production? [y/n]: ", end="")
      response = input().strip().lower()
      return response == "y"
  ```

### Task 3: Generate SQL export file (AC: 5, 6, 7, 8, 9, 13)
- [ ] Implement SQL generation with transaction wrapper
  ```python
  from datetime import datetime

  async def generate_sql_export(
      book_id: UUID,
      session: AsyncSession,
      output_dir: Path
  ) -> Path:
      """Generate SQL export file for book."""

      # Fetch all data
      book = await session.get(Book, book_id)
      chunks_query = select(Chunk).where(Chunk.book_id == book_id)
      chunks_result = await session.execute(chunks_query)
      chunks = chunks_result.scalars().all()

      screenshots_query = select(Screenshot).where(Screenshot.book_id == book_id)
      screenshots_result = await session.execute(screenshots_query)
      screenshots = screenshots_result.scalars().all()

      # Get embedding config
      config_query = select(EmbeddingConfig).where(
          EmbeddingConfig.id == chunks[0].embedding_config_id
      )
      config_result = await session.execute(config_query)
      embedding_config = config_result.scalar_one_or_none()

      # Generate filename
      timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
      filename = f"{book_id}_{timestamp}.sql"
      output_path = output_dir / filename

      # Write SQL file
      output_dir.mkdir(parents=True, exist_ok=True)

      with open(output_path, "w") as f:
          # Header
          f.write(f"-- Minerva Knowledge Base Export\n")
          f.write(f"-- Book: {book.title}\n")
          f.write(f"-- Exported: {datetime.now().isoformat()}\n")
          f.write(f"-- Total Chunks: {len(chunks)}\n\n")

          # Transaction wrapper
          f.write("BEGIN;\n\n")

          # Embedding config (with ON CONFLICT for idempotency)
          if embedding_config:
              f.write("-- Embedding Configuration\n")
              f.write(
                  f"INSERT INTO embedding_configs (id, model_name, model_version, dimensions, is_active, created_at)\n"
                  f"VALUES ('{embedding_config.id}', '{embedding_config.model_name}', "
                  f"'{embedding_config.model_version or 'v1'}', {embedding_config.dimensions}, "
                  f"{embedding_config.is_active}, '{embedding_config.created_at.isoformat()}')\n"
                  f"ON CONFLICT (id) DO NOTHING;\n\n"
              )

          # Book record (exclude local paths, use ON CONFLICT)
          f.write("-- Book Record\n")
          f.write(
              f"INSERT INTO books (id, title, author, kindle_url, total_screenshots, "
              f"capture_date, ingestion_status, metadata, created_at, updated_at)\n"
              f"VALUES ('{book.id}', '{book.title.replace("'", "''")}', "
              f"'{book.author.replace("'", "''") if book.author else 'NULL'}', "
              f"'{book.kindle_url}', {book.total_screenshots or 'NULL'}, "
              f"'{book.capture_date.isoformat()}', 'completed', "
              f"'{json.dumps(book.metadata) if book.metadata else 'NULL'}', "
              f"'{book.created_at.isoformat()}', '{book.updated_at.isoformat()}')\n"
              f"ON CONFLICT (id) DO UPDATE SET updated_at = EXCLUDED.updated_at;\n\n"
          )

          # Screenshots metadata (file_path explicitly NULL)
          f.write("-- Screenshot Metadata (file_path NULL for production)\n")
          for screenshot in screenshots:
              f.write(
                  f"INSERT INTO screenshots (id, book_id, sequence_number, file_path, "
                  f"screenshot_hash, captured_at)\n"
                  f"VALUES ('{screenshot.id}', '{screenshot.book_id}', "
                  f"{screenshot.sequence_number}, NULL, '{screenshot.screenshot_hash}', "
                  f"'{screenshot.captured_at.isoformat()}')\n"
                  f"ON CONFLICT (id) DO NOTHING;\n"
              )
          f.write("\n")

          # Chunks with embeddings
          f.write("-- Text Chunks with Embeddings\n")
          for chunk in chunks:
              # Convert embedding to PostgreSQL array format
              embedding_str = "{" + ",".join(map(str, chunk.embedding)) + "}"

              f.write(
                  f"INSERT INTO chunks (id, book_id, screenshot_ids, chunk_sequence, "
                  f"chunk_text, chunk_token_count, embedding_config_id, embedding, "
                  f"vision_model, created_at)\n"
                  f"VALUES ('{chunk.id}', '{chunk.book_id}', "
                  f"ARRAY{str(chunk.screenshot_ids).replace('[', '{').replace(']', '}')}, "
                  f"{chunk.chunk_sequence}, '{chunk.chunk_text.replace("'", "''")}', "
                  f"{chunk.chunk_token_count}, '{chunk.embedding_config_id}', "
                  f"'{embedding_str}'::vector, '{chunk.vision_model}', "
                  f"'{chunk.created_at.isoformat()}')\n"
                  f"ON CONFLICT (id) DO NOTHING;\n"
              )

          # Commit transaction
          f.write("\nCOMMIT;\n")

      return output_path
  ```

### Task 4: Display success message (AC: 10)
- [ ] Print import instructions
  ```python
  def display_success_message(export_path: Path, report: dict):
      """Display success message with import instructions."""
      console = Console()

      console.print(f"\n[bold green]✅ Export Complete![/bold green]\n")
      console.print(f"📄 Export File: {export_path.absolute()}")
      console.print(f"📦 Size: {report['estimated_size_mb']} MB")
      console.print(f"📚 Book: {report['title']}")
      console.print(f"📝 Chunks: {report['total_chunks']}\n")

      console.print("[bold cyan]Import Instructions:[/bold cyan]")
      console.print(f"  1. Copy {export_path.name} to production server")
      console.print(f"  2. Run: psql $PRODUCTION_DATABASE_URL -f {export_path.name}")
      console.print(f"  3. Verify: SELECT COUNT(*) FROM chunks WHERE book_id = '{report['book_id']}';")
      console.print()
  ```

### Task 5: Add --all flag support (AC: 12)
- [ ] Implement batch export for all completed books
  ```python
  async def export_all_books(session: AsyncSession, output_dir: Path):
      """Export all completed books."""

      # Find all completed books
      query = select(Book).where(Book.ingestion_status == "completed")
      result = await session.execute(query)
      books = result.scalars().all()

      console = Console()
      console.print(f"\nFound {len(books)} completed books to export\n")

      for book in books:
          try:
              # Validate and export
              report = await validate_and_report(book.id, session)
              export_path = await generate_sql_export(book.id, session, output_dir)

              console.print(f"✅ Exported: {book.title} → {export_path.name}")

          except Exception as e:
              console.print(f"❌ Failed to export {book.title}: {e}")

      console.print(f"\n✨ Batch export complete! Files in: {output_dir.absolute()}\n")
  ```

### Task 6: Test export/import cycle (AC: 11)
- [ ] Create test script `scripts/test_export_import.py`
- [ ] Validate export → import → query workflow
  ```python
  async def test_export_import():
      """Test full export/import cycle."""

      # Export test book
      book_id = UUID("...")  # Test book ID
      async with AsyncSessionLocal() as session:
          report = await validate_and_report(book_id, session)
          export_path = await generate_sql_export(book_id, session, Path("exports"))

      print(f"✅ Export complete: {export_path}")

      # Import to test production database
      import subprocess
      result = subprocess.run(
          ["psql", os.getenv("TEST_PRODUCTION_DATABASE_URL"), "-f", str(export_path)],
          capture_output=True,
          text=True
      )

      if result.returncode != 0:
          print(f"❌ Import failed: {result.stderr}")
          return

      print("✅ Import complete")

      # Verify chunks in production DB
      prod_conn_string = os.getenv("TEST_PRODUCTION_DATABASE_URL")
      async with create_async_engine(prod_conn_string).connect() as conn:
          count_query = text(f"SELECT COUNT(*) FROM chunks WHERE book_id = '{book_id}'")
          result = await conn.execute(count_query)
          chunk_count = result.scalar()

      print(f"✅ Verified: {chunk_count} chunks in production DB")

      # Test vector search
      search_query = text(
          "SELECT chunk_text, 1 - (embedding <-> '[...]'::vector) AS similarity "
          "FROM chunks WHERE book_id = :book_id ORDER BY similarity DESC LIMIT 5"
      )
      # ... test search ...

      print("✅ All validation checks passed!")
  ```

## Dev Notes

### Architecture Context

**CLI Framework (Source: docs/architecture/tech-stack.md)**
- Typer 0.9+: Modern CLI framework with type hints
- Rich 13.7+: Beautiful terminal UI (tables, progress bars, colors)
- Click integration: Typer built on Click (same patterns as Flask CLI)

**Export Strategy**
- SQL INSERT statements (not pg_dump) for portability
- ON CONFLICT clauses for idempotency (safe re-export)
- Transaction wrapper (BEGIN/COMMIT) for atomic import
- Explicit NULL for screenshot file_path (security)

**Data Exclusions (Security)**
- Screenshot file_path: Always NULL in production (screenshots never exported)
- Local paths: Kindle URLs preserved (reference only)
- Secrets: No API keys or credentials in export
- Comment in SQL: "file_path NULL for production security"

**Idempotency Pattern**
- `INSERT ... ON CONFLICT (id) DO NOTHING` for chunks/screenshots
- `INSERT ... ON CONFLICT (id) DO UPDATE SET updated_at = ...` for books
- Safe to re-export: No duplicates, updates timestamps only
- Embedding config: ON CONFLICT DO NOTHING (immutable once created)

### Previous Story Insights

**Story 2.4 (Pipeline):**
- Rich Progress bars for visual feedback
- Structured logging for tracking
- Transaction safety with rollback

**Patterns to Reuse:**
- Rich Console for beautiful output
- Validation before action (check status, embeddings)
- Clear error messages for user

### Implementation Considerations

**SQL Generation Challenges:**
- Escaping single quotes: Use `str.replace("'", "''")` for SQL strings
- Vector format: PostgreSQL `{0.1,0.2,...}::vector` syntax
- UUID arrays: Convert Python list to PostgreSQL array `{uuid1,uuid2}`
- NULL handling: Write literal `NULL` not Python `None`

**Export File Size:**
- 1 chunk ~500-800 chars text + 1536 floats embedding = ~7KB
- 100 chunks ≈ 700KB
- 1000 chunks ≈ 7MB
- Warn if >100MB (compression recommended)

**Performance:**
- Single query per table (not per-row INSERTs)
- But write row-by-row for readability
- Trade-off: Slower export (acceptable) for readable SQL

**Embedding Config Handling:**
- Check if config exists in production (query first)
- If not exists: Include CREATE statement
- If exists: Skip (ON CONFLICT DO NOTHING)
- Ensures model consistency across environments

### Dependencies

**New Files Created:**
- `minerva/core/export/export_service.py` (export logic)
- `minerva/cli/export.py` or add to `minerva/cli/app.py` (CLI command)
- `scripts/test_export_import.py` (validation script)

**Existing Files Used:**
- `minerva/db/models/*` (Book, Chunk, Screenshot, EmbeddingConfig)
- `minerva/db/session.py` (AsyncSessionLocal)
- `minerva/config.py` (settings)

**External Dependencies (already in pyproject.toml):**
- `typer = "^0.9.0"` (CLI framework)
- `rich = "^13.7.0"` (terminal UI)
- `sqlalchemy[asyncio] = "^2.0.0"` (database)

### Success Criteria

**Functional:**
- [x] `minerva export --book-id <uuid>` command works
- [x] Validates book status and embeddings
- [x] Displays pre-export report
- [x] Prompts for user confirmation
- [x] Generates SQL with INSERT statements
- [x] Transaction wrapper (BEGIN/COMMIT)
- [x] Screenshot file_path = NULL
- [x] ON CONFLICT for idempotency
- [x] `--all` flag exports all completed books

**Testing:**
- [x] Export test book → SQL file created
- [x] Import to test production DB → Success
- [x] Query chunks in production → Embeddings intact
- [x] Re-export same book → No duplicates

**Documentation:**
- [x] Success message with import instructions
- [x] Clear error messages for validation failures
- [x] Comments in SQL explaining NULL file_path

### Next Steps (Story 3.6)

After Story 3.5 completion:
- **Story 3.6**: Production database setup
  - Alembic migration for production schema
  - Import validation script
  - Performance testing with realistic data
  - Security: Limited DB permissions

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 1.0 | Initial story creation | Sarah (PO) |
| 2025-10-07 | 2.0 | Comprehensive redraft with architecture context | Bob (SM) |

## Dev Agent Record

### Agent Model Used
_To be filled by Dev Agent_

### Debug Log References
_To be filled by Dev Agent_

### Completion Notes
_To be filled by Dev Agent_

### File List
_To be filled by Dev Agent_

## QA Results
_To be filled by QA Agent_
