# Story 3.6: Production Database Setup and Import Validation

## Status
Ready for Review

## Story

**As a** developer,
**I want** production database schema and validated import process,
**so that** exported knowledge can be safely imported to production environment.

## Acceptance Criteria

1. Production database setup script created: alembic migration applicable to production DB
2. Production schema identical to local schema except: screenshots.file_path nullable (always NULL in production), appropriate indexes created
3. Import validation script created: validates SQL export file before import (syntax check, required tables exist, no file paths included)
4. Import instructions documented in README: connection to production DB, running export SQL, verification queries
5. Test production database created (mpp_minerva_prod) on local machine for validation
6. Successfully imports exported SQL to test production DB: all books, chunks, embeddings present
7. Post-import validation queries: count chunks, verify embeddings non-null, check book status
8. Production DB performance validated: vector search queries <200ms with realistic data volume (1000+ chunks)
9. Rollback procedure documented: how to safely remove imported book if issues discovered
10. Security validated: production DB user has limited permissions (INSERT, SELECT only - no DELETE or ALTER)
11. Connection string for production DB configurable via PRODUCTION_DATABASE_URL environment variable

## Tasks / Subtasks

### Task 1: Create production DB migration (AC: 1, 2)
- [ ] Update Alembic migration for production compatibility
  - Screenshots.file_path: Change to nullable (NULL default in production)
  - All other tables identical to local schema
  - Includes all indexes (especially IVFFlat for vector search)

### Task 2: Create import validation script (AC: 3)
- [ ] Create `scripts/validate_export.py` to check SQL file before import
  ```python
  def validate_export_file(export_path: Path) -> tuple[bool, list[str]]:
      """Validate SQL export file for production import."""
      warnings = []

      with open(export_path) as f:
          sql_content = f.read()

      # Check for file paths (security)
      if "file_path" in sql_content and "NULL" not in sql_content.split("file_path")[1].split("\n")[0]:
          warnings.append("❌ file_path contains non-NULL values (security risk)")
          return False, warnings

      # Check for BEGIN/COMMIT
      if "BEGIN;" not in sql_content or "COMMIT;" not in sql_content:
          warnings.append("⚠️  Missing transaction wrapper")

      # Check for required tables
      required_tables = ["books", "chunks", "screenshots", "embedding_configs"]
      for table in required_tables:
          if table not in sql_content:
              warnings.append(f"⚠️  Missing {table} table")

      # Check for ON CONFLICT (idempotency)
      if "ON CONFLICT" not in sql_content:
          warnings.append("⚠️  Missing ON CONFLICT clauses (not idempotent)")

      return True, warnings
  ```

### Task 3: Document import process and rollback (AC: 4, 9)
- [ ] Add to README.md section "Production Database Import"
  ```markdown
  ## Production Database Import

  ### Prerequisites
  - Production database with Minerva schema (run Alembic migrations)
  - Export SQL file from local database (see Story 3.5)
  - Database connection string: PRODUCTION_DATABASE_URL

  ### Import Steps
  1. Validate export file:
     ```bash
     python scripts/validate_export.py exports/book_xyz.sql
     ```

  2. Backup production DB (if data exists):
     ```bash
     pg_dump $PRODUCTION_DATABASE_URL > backup_$(date +%Y%m%d).sql
     ```

  3. Import SQL file:
     ```bash
     psql $PRODUCTION_DATABASE_URL -f exports/book_xyz.sql
     ```

  4. Verify import:
     ```sql
     SELECT COUNT(*) FROM chunks WHERE book_id = 'uuid';
     SELECT COUNT(*) FROM chunks WHERE embedding IS NULL;
     ```

  ### Rollback Procedure
  If import fails or data is incorrect:

  1. Delete imported book:
     ```sql
     DELETE FROM chunks WHERE book_id = 'uuid';
     DELETE FROM screenshots WHERE book_id = 'uuid';
     DELETE FROM books WHERE id = 'uuid';
     ```

  2. Or restore from backup:
     ```bash
     psql $PRODUCTION_DATABASE_URL < backup_20251007.sql
     ```
  ```

### Task 4: Create test production DB (AC: 5)
- [ ] Setup local test production database
  ```bash
  # Create test production database
  createdb mpp_minerva_prod

  # Run Alembic migrations
  PRODUCTION_DATABASE_URL=postgresql://localhost/mpp_minerva_prod alembic upgrade head

  # Verify schema
  psql mpp_minerva_prod -c "\dt"  # List tables
  psql mpp_minerva_prod -c "\d chunks"  # Check chunks table schema
  ```

### Task 5: Test import workflow (AC: 6, 7)
- [ ] Import test book and verify
  ```python
  # Export from local
  minerva export --book-id <test-uuid>

  # Validate export
  python scripts/validate_export.py exports/*.sql

  # Import to test production DB
  psql postgresql://localhost/mpp_minerva_prod -f exports/*.sql

  # Verify
  async with create_async_engine(TEST_PROD_URL).connect() as conn:
      # Count chunks
      result = await conn.execute(text("SELECT COUNT(*) FROM chunks"))
      chunk_count = result.scalar()

      # Check embeddings
      result = await conn.execute(text("SELECT COUNT(*) FROM chunks WHERE embedding IS NULL"))
      null_embeddings = result.scalar()

      assert null_embeddings == 0, "Found chunks with NULL embeddings"
      print(f"✅ Imported {chunk_count} chunks with embeddings")
  ```

### Task 6: Validate performance (AC: 8)
- [ ] Test vector search performance
  ```python
  # Generate test embedding
  from minerva.core.ingestion.embedding_generator import EmbeddingGenerator

  async with create_async_engine(TEST_PROD_URL).begin() as conn:
      gen = EmbeddingGenerator(session=conn)
      query_embedding = await gen.generate_embeddings(["test query"])

      # Test search query
      import time
      start = time.time()

      search_query = text("""
          SELECT chunk_id, chunk_text, 1 - (embedding <-> :query_vec::vector) AS similarity
          FROM chunks
          ORDER BY similarity DESC
          LIMIT 10
      """)

      result = await conn.execute(search_query, {"query_vec": str(query_embedding[0])})
      rows = result.fetchall()

      elapsed_ms = (time.time() - start) * 1000

      assert elapsed_ms < 200, f"Search too slow: {elapsed_ms}ms"
      print(f"✅ Search completed in {elapsed_ms:.2f}ms")
  ```

### Task 7: Configure security (AC: 10, 11)
- [ ] Create limited production DB user
  ```sql
  -- On production database
  CREATE USER minerva_api WITH PASSWORD 'secure-password';

  -- Grant limited permissions
  GRANT CONNECT ON DATABASE mpp_minerva_prod TO minerva_api;
  GRANT SELECT, INSERT ON ALL TABLES IN SCHEMA public TO minerva_api;
  GRANT USAGE ON ALL SEQUENCES IN SCHEMA public TO minerva_api;

  -- Explicitly REVOKE dangerous permissions
  REVOKE DELETE, UPDATE, TRUNCATE, ALTER ON ALL TABLES IN SCHEMA public FROM minerva_api;

  -- Verify permissions
  \dp  -- List permissions
  ```
- [ ] Document PRODUCTION_DATABASE_URL format in .env.example
  ```
  # Production Database (read-only API user)
  PRODUCTION_DATABASE_URL=postgresql://minerva_api:password@host:5432/mpp_minerva_prod
  ```

## Dev Notes

### Architecture Context

**Production vs Local Schema (Source: docs/architecture/database-schema.md)**
- **Local**: screenshots.file_path contains actual paths (e.g., `/path/to/screenshot.png`)
- **Production**: screenshots.file_path always NULL (no screenshots in production)
- Reason: Security (don't expose local file paths), Cost (no screenshot storage in production)
- All other fields identical (id, sequence_number, screenshot_hash preserved for lineage)

**Migration Strategy**
- Use same Alembic migrations for both environments
- Schema differences handled by export script (sets file_path = NULL)
- Production migration includes all indexes (IVFFlat critical for performance)

**Security Model (Source: docs/architecture/security.md)**
- Production DB user: Limited to INSERT, SELECT only
- No DELETE/UPDATE (prevent accidental data loss)
- No ALTER (prevent schema changes)
- API uses read-only connection (can't modify data after import)

**Performance Requirements**
- IVFFlat index mandatory (vector search <200ms)
- Index parameters: `lists = 100` (optimal for 10k-100k vectors)
- Re-index if chunks > 100k (increase lists parameter)

### Previous Story Insights

**Story 3.5 (Export Script):**
- SQL export with ON CONFLICT for idempotency
- Transaction wrapper (BEGIN/COMMIT) for atomic import
- file_path explicitly NULL in export
- Validation before export (status = "completed", embeddings present)

**Import Validation Pattern:**
- Check SQL syntax (BEGIN/COMMIT present)
- Verify file_path = NULL (security)
- Confirm ON CONFLICT (idempotency)
- Test in local production DB before real production

### Implementation Considerations

**Test Production Database:**
- Local PostgreSQL instance (mpp_minerva_prod)
- Identical schema to real production
- Used for import testing before production deploy
- Can reset/recreate easily (dropdb/createdb)

**Import Verification Queries:**
```sql
-- Check chunk count
SELECT book_id, COUNT(*) as chunk_count FROM chunks GROUP BY book_id;

-- Check for NULL embeddings
SELECT COUNT(*) FROM chunks WHERE embedding IS NULL;

-- Check vector dimensions
SELECT id, array_length(embedding, 1) FROM chunks LIMIT 1;

-- Test vector search
SELECT 1 - (embedding <-> '[0.1, 0.2, ...]'::vector) FROM chunks LIMIT 5;
```

**Rollback Scenarios:**
1. **Import fails mid-transaction**: Automatic rollback (transaction wrapper)
2. **Import succeeds but data wrong**: Manual DELETE or restore from backup
3. **Performance issues**: Check IVFFlat index exists (`\d+ chunks`)

**Security Best Practices:**
- Never commit production credentials to git
- Use environment variables for all DB connections
- Production DB user has minimal permissions
- Audit logs for all production imports (who, when, what)

### Dependencies

**New Files Created:**
- `scripts/validate_export.py` (SQL validation)
- `scripts/setup_test_prod_db.sh` (test DB setup)
- `docs/PRODUCTION_IMPORT.md` (import instructions)

**Existing Files Used:**
- `alembic/versions/*.py` (migrations)
- `minerva/db/session.py` (connection management)
- Alembic configuration (alembic.ini)

**External Dependencies:**
- PostgreSQL 15+ with pgvector (same as local)
- psql CLI tool (for import)
- Python asyncpg (for validation scripts)

### Success Criteria

**Functional:**
- [x] Alembic migration works on production DB
- [x] Import validation script catches security issues
- [x] Test production DB created locally
- [x] Export imports successfully
- [x] All chunks have non-NULL embeddings
- [x] Vector search works in production DB

**Performance:**
- [x] Vector search <200ms with 1000+ chunks
- [x] IVFFlat index created and used

**Security:**
- [x] Production user has limited permissions
- [x] No DELETE/UPDATE access
- [x] PRODUCTION_DATABASE_URL configured

**Documentation:**
- [x] Import instructions in README
- [x] Rollback procedure documented
- [x] Verification queries documented

### Next Steps (Story 3.7)

After Story 3.6 completion:
- **Story 3.7**: Deploy API to production (Railway/Fly.io)
- Dockerfile for lightweight API image (no Playwright)
- Environment variables (DATABASE_URL, OPENAI_API_KEY)
- HTTPS/SSL configuration
- Monitoring and logging

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 1.0 | Initial story creation | Sarah (PO) |
| 2025-10-07 | 2.0 | Comprehensive redraft with architecture context | Bob (SM) |

## Dev Agent Record

### Agent Model Used
_To be filled by Dev Agent_

### Debug Log References
_To be filled by Dev Agent_

### Completion Notes
_To be filled by Dev Agent_

### File List
_To be filled by Dev Agent_

## QA Results
_To be filled by QA Agent_
